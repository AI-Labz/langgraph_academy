{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "We've build an agent with memory:\n",
    "\n",
    "- `act` - tool node executes a tool\n",
    "- `observe` - pass the tool node outputs back to the LLM\n",
    "- `reason` - LLM  reasons about to do next (e.g., call another tool)\n",
    "- `persist state` - using in memory checkpointer for long-running conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "Now we'll cover how to actually deploy your agent!\n",
    "\n",
    "We've cover deployment locally and to `LangGraph Cloud`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few concepts to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LangGraph`\n",
    "\n",
    "- Python and Javascript library\n",
    "- Allows creation of agentic workflows\n",
    "\n",
    "`LangGraph  API`\n",
    "\n",
    "- Blundles the graph code\n",
    "- Provides a task queue for managing asynchronous operations\n",
    "- Offers persistence for maintaining state across interactions\n",
    "\n",
    "`LangGraph Cloud`\n",
    "\n",
    "- Hosted service for the LangGraph API\n",
    "- Allows deployment of graphs from GitHub repositories\n",
    "- Provides monitoring, tracing, and API documentation for deployed graphs\n",
    "- Accessible via a unique URL for each deployment\n",
    "\n",
    "`LangGraph Studio`\n",
    "\n",
    "- Integrated Development Environment (IDE) for LangGraph applications\n",
    "- Uses the API as its back-end, allowing reaal-time testing and exploration of graphs\n",
    "- Can be run locally or with cloud-deployment\n",
    "\n",
    "`LangGraph SDK`\n",
    "\n",
    "- Python library for programmatically interacting with LangGraph graphs\n",
    "- Provides a consistent interface for working with graphs, wheter served locally or in the cloud.\n",
    "- Allows creation of clients, access to assistants, thread management, and execution of runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Locally\n",
    "\n",
    "We can easily connect with graphs that are served locally in LangGraph Studio!\n",
    "\n",
    "We do this via the `url` provided in API field in the image below.\n",
    "\n",
    "<center><img src=\"../../../assets/langgraph-cli.png\" width=900 alt=\"langgraph-cli\"></img></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'assistant_id': '166c0d22-706c-5a5b-9027-ea37f7308a85',\n",
       "  'graph_id': 'react',\n",
       "  'config': {},\n",
       "  'metadata': {'created_by': 'system'},\n",
       "  'name': 'react',\n",
       "  'created_at': '2024-12-29T16:26:28.065150+00:00',\n",
       "  'updated_at': '2024-12-29T16:26:28.065150+00:00',\n",
       "  'version': 1},\n",
       " {'assistant_id': '228f9934-0cdd-5383-92c8-ee8422522cc2',\n",
       "  'graph_id': 'router',\n",
       "  'config': {},\n",
       "  'metadata': {'created_by': 'system'},\n",
       "  'name': 'router',\n",
       "  'created_at': '2024-12-29T16:26:27.689297+00:00',\n",
       "  'updated_at': '2024-12-29T16:26:27.689297+00:00',\n",
       "  'version': 1},\n",
       " {'assistant_id': '608f7b9c-04e5-5cf2-a3f7-ba75a3719808',\n",
       "  'graph_id': 'chain',\n",
       "  'config': {},\n",
       "  'metadata': {'created_by': 'system'},\n",
       "  'name': 'chain',\n",
       "  'created_at': '2024-12-29T16:26:27.316657+00:00',\n",
       "  'updated_at': '2024-12-29T16:26:27.316657+00:00',\n",
       "  'version': 1},\n",
       " {'assistant_id': '5e0f6b46-d00f-5af4-87dc-ea2105ff526d',\n",
       "  'graph_id': 'simple',\n",
       "  'config': {},\n",
       "  'metadata': {'created_by': 'system'},\n",
       "  'name': 'simple',\n",
       "  'created_at': '2024-12-26T19:57:43.536734+00:00',\n",
       "  'updated_at': '2024-12-26T19:57:43.536734+00:00',\n",
       "  'version': 1}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()\n",
    "assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assistant_id': '166c0d22-706c-5a5b-9027-ea37f7308a85',\n",
       " 'graph_id': 'react',\n",
       " 'config': {},\n",
       " 'metadata': {'created_by': 'system'},\n",
       " 'name': 'react',\n",
       " 'created_at': '2024-12-29T16:26:28.065150+00:00',\n",
       " 'updated_at': '2024-12-29T16:26:28.065150+00:00',\n",
       " 'version': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sellectthe ReAct Agent\n",
    "assistant = assistants[0]\n",
    "assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a thread to manage and track the conversation\n",
    "thread = await client.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Multiply 2 by 3', 'additional_kwargs': {'additional_kwargs': {}, 'response_metadata': {}, 'example': False}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'de063df0-8368-4a9e-bd43-b03eb12de6db', 'example': False}\n",
      "{'content': '', 'additional_kwargs': {}, 'response_metadata': {'model': 'llama3.1-tool', 'created_at': '2025-01-02T14:13:46.0557478Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6241571300, 'load_duration': 4814522700, 'prompt_eval_count': 375, 'prompt_eval_duration': 275000000, 'eval_count': 22, 'eval_duration': 635000000, 'message': {'role': 'assistant', 'content': '', 'images': None, 'tool_calls': None}}, 'type': 'ai', 'name': None, 'id': 'run-80fff94e-aaa4-4bbf-895d-1910e1a8e353-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': '8d1667d3-bfea-44aa-87ba-a7508e384640', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 375, 'output_tokens': 22, 'total_tokens': 397}}\n",
      "{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'fb25b0bd-dab1-4253-ad0e-2fd02f33fdc2', 'tool_call_id': '8d1667d3-bfea-44aa-87ba-a7508e384640', 'artifact': None, 'status': 'success'}\n",
      "{'content': 'The result of multiplying 2 by 3 is 6.', 'additional_kwargs': {}, 'response_metadata': {'model': 'llama3.1-tool', 'created_at': '2025-01-02T14:13:46.5336832Z', 'done': True, 'done_reason': 'stop', 'total_duration': 464602300, 'load_duration': 23129900, 'prompt_eval_count': 161, 'prompt_eval_duration': 44000000, 'eval_count': 14, 'eval_duration': 394000000, 'message': {'role': 'assistant', 'content': 'The result of multiplying 2 by 3 is 6.', 'images': None, 'tool_calls': None}}, 'type': 'ai', 'name': None, 'id': 'run-887f9172-25d7-482a-9901-3f36492394c1-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 161, 'output_tokens': 14, 'total_tokens': 175}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Input\n",
    "message = {\"messages\": [HumanMessage(content=\"Multiply 2 by 3\")]}\n",
    "\n",
    "# Stream\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"], assistant[\"assistant_id\"], input=message, stream_mode=\"values\"\n",
    "):\n",
    "    if chunk.data and chunk.event != \"metadata\":\n",
    "        print(chunk.data[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with the Cloud\n",
    "\n",
    "We can deploy to Cloud via LangSmith"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-academy-GSfDBkeK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
